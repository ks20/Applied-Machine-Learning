{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW3_Sentiment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9U6PxYL4419",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import sys\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import confusion_matrix\n",
        "# np.set_printoptions(threshold=sys.maxsize)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12kf0f2x4LGH",
        "colab_type": "code",
        "outputId": "f22a6139-ca3d-4fbf-c2aa-c35eee3d4a58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VQ2avGM4i2R",
        "colab_type": "text"
      },
      "source": [
        "# A: Dataset Analysis\n",
        "Download Sentiment Labelled Sentences Data Set. There are three data files under the root\n",
        "folder. yelp_labelled.txt, amazon_cells_labelled.txt and imdb_labelled.txt. Parse each file\n",
        "with the specifications in readme.txt. Are the labels balanced? If not, what’s the ratio between\n",
        "the two labels? Explain how you process these files.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBgCvB2B5WrK",
        "colab_type": "code",
        "outputId": "266dda85-883c-4ab7-bf68-77004454647b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "path_prefix = 'drive/My Drive/CS 5785/HW3/sentiment labelled sentences'\n",
        "data_sets = [\"amazon_cells_labelled.txt\", \"imdb_labelled.txt\", \"yelp_labelled.txt\"]\n",
        "\n",
        "def parse_data_set(filename):\n",
        "    positives = []\n",
        "    negatives = []\n",
        "    print(\"Parsing dataset: {}\".format(filename))\n",
        "    with open('{}/{}'.format(path_prefix, filename)) as f:\n",
        "        count = 0\n",
        "        pos_count = 0\n",
        "        neg_count = 0\n",
        "        lines = f.readlines()\n",
        "        for line in lines:\n",
        "            data = line.strip().split('\\t')\n",
        "            value = data[1]\n",
        "            review = data[0]\n",
        "            if value == '1':\n",
        "                positives.append((value, review))\n",
        "                pos_count += 1\n",
        "            else:\n",
        "                negatives.append((value, review))\n",
        "                neg_count += 1\n",
        "            count = count + 1\n",
        "        print(\"Total lines: {}\".format(count))\n",
        "\n",
        "    print(\"Positive count: {}\".format(len(positives)))\n",
        "    print(\"Negative count: {}\".format(len(negatives)))\n",
        "    return positives, negatives\n",
        "\n",
        "amazon_positives, amazon_negatives = parse_data_set(\"amazon_cells_labelled.txt\")\n",
        "imdb_positives, imdb_negatives = parse_data_set(\"imdb_labelled.txt\")\n",
        "yelp_positives, yelp_negatives = parse_data_set(\"yelp_labelled.txt\")"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parsing dataset: amazon_cells_labelled.txt\n",
            "Total lines: 1000\n",
            "Positive count: 500\n",
            "Negative count: 500\n",
            "Parsing dataset: imdb_labelled.txt\n",
            "Total lines: 1000\n",
            "Positive count: 500\n",
            "Negative count: 500\n",
            "Parsing dataset: yelp_labelled.txt\n",
            "Total lines: 1000\n",
            "Positive count: 500\n",
            "Negative count: 500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcB5BCztBC9I",
        "colab_type": "text"
      },
      "source": [
        "# B: Preprocessing Strategy\n",
        "Lowercase all words: All caps can be indicative for both directions. Lowercase improved accuracy.\n",
        "\n",
        "Lemmatization (running, runs, run all same): NO -> Made accuracy worse\n",
        "**TODO:** Find better stemmer\n",
        "\n",
        "Strip punctuation: Kinda, need exclamation marks and such, but periods can be stripped. Punctuation should be a separate bucket in the feature vector and not part of the word it's next to.  Yeah, would probaby be better off stripping punctuation because there are many malformed sentences that hurt BoW word counts\n",
        "Yes, ended up stripping punctuation\n",
        "\n",
        "Strip stop words: Yes, did this.\n",
        "\n",
        "Initially, started out without pre processing. Then, stripped punctuation, and stop words.  Initially had lemmatiziation, but accuracy was worse.  Then, lowercased all words, which helped as well.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UH_aZPdd6sKS",
        "colab_type": "code",
        "outputId": "0f3158c8-f9e1-4cca-a12c-55f86027f149",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "import string\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Wordnet stemmer\n",
        "nltk.download(\"wordnet\")\n",
        "wnl = WordNetLemmatizer()\n",
        "l = nltk.stem.snowball.EnglishStemmer()\n",
        "\n",
        "# Lemmatization of the data using NLTK English Snowball Stemmer\n",
        "def lemmatize(w):\n",
        "    sentence = w.split(' ')\n",
        "    new_sentence = []\n",
        "    for s in sentence:\n",
        "        tmp = wnl.lemmatize(s)\n",
        "        new_sentence.append(l.stem(tmp))\n",
        "    return \" \".join(new_sentence)\n",
        "\n",
        "def remove_stop_words(s):\n",
        "    word_tokens = word_tokenize(s)\n",
        "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
        "    return \" \".join(filtered_sentence)\n",
        "\n",
        "# Remove punctuation\n",
        "def remove_punctuation(s):\n",
        "    # print(\"Pre punctuation removal: {}\".format(s[1]))\n",
        "    # print(s[1])\n",
        "    clean = re.sub(r\"[,.;@#?!&$]+\\ *\", \" \", s)\n",
        "    # print(\"Post punctuation removal: {}\".format(clean))\n",
        "    return clean\n",
        "\n",
        "# Almost need to replace punctuation with space b/c of bad typing also\n",
        "s = 'Tied to charger for conversations lasting more than 45 minutes.MAJOR PROBLEMS!! Can\\'t stop won\\'t stop!'\n",
        "remove_punctuation(amazon_negatives[1][1])\n",
        "print(remove_punctuation(\"Then, as if i hadn't wasted enough of my time, they\"))\n",
        "\n",
        "print(remove_stop_words(\"testing this as a stop word remover that should take out things like The\"))\n",
        "\n",
        "print(lemmatize(\"Running with this\"))\n",
        "\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Then as if i hadn't wasted enough of my time they\n",
            "testing stop word remover take things like The\n",
            "run with this\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0R8sySK8TFA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Perform the pre processing on the dataset\n",
        "all_data = amazon_negatives + amazon_positives + imdb_negatives + imdb_positives + yelp_negatives + yelp_positives\n",
        "\n",
        "# Remove punctuation\n",
        "# print(all_data[1])\n",
        "# list(map(remove_punctuation, all_data))\n",
        "# Amazon pre processing\n",
        "for idx, d in enumerate(amazon_negatives):\n",
        "    w = d[1]\n",
        "    # print(w)\n",
        "    w = w.lower()\n",
        "    # print(w)\n",
        "    w = remove_punctuation(w)\n",
        "    # print(w)\n",
        "    w = remove_stop_words(w)\n",
        "    # print(w)\n",
        "    w = lemmatize(w)\n",
        "    # print(w)\n",
        "    amazon_negatives[idx] = (amazon_negatives[idx][0], w)\n",
        "for idx, d in enumerate(amazon_positives):\n",
        "    w = d[1]\n",
        "    # print(w)\n",
        "    w = remove_punctuation(w)\n",
        "    # print(w)\n",
        "    w = remove_stop_words(w)\n",
        "    # print(w)\n",
        "    w = lemmatize(w)\n",
        "    # print(w)\n",
        "    # w = w.lower()\n",
        "    amazon_positives[idx] = (amazon_positives[idx][0], w)\n",
        "\n",
        "\n",
        "# IMDB Preprocessing\n",
        "for idx, d in enumerate(imdb_negatives):\n",
        "    w = d[1]\n",
        "    # print(w)\n",
        "    w = remove_punctuation(w)\n",
        "    # print(w)\n",
        "    w = remove_stop_words(w)\n",
        "    # print(w)\n",
        "    w = lemmatize(w)\n",
        "    # print(w)\n",
        "    # w = w.lower()\n",
        "    imdb_negatives[idx] = (imdb_negatives[idx][0], w)\n",
        "for idx, d in enumerate(imdb_positives):\n",
        "    w = d[1]\n",
        "    # print(w)\n",
        "    w = remove_punctuation(w)\n",
        "    # print(w)\n",
        "    w = remove_stop_words(w)\n",
        "    # print(w)\n",
        "    w = lemmatize(w)\n",
        "    # print(w)\n",
        "    # w = w.lower()\n",
        "    imdb_positives[idx] = (imdb_positives[idx][0], w)\n",
        "\n",
        "\n",
        "# Yelp preprocessing\n",
        "for idx, d in enumerate(yelp_negatives):\n",
        "    w = d[1]\n",
        "    # print(w)\n",
        "    w = remove_punctuation(w)\n",
        "    # print(w)\n",
        "    w = remove_stop_words(w)\n",
        "    # print(w)\n",
        "    w = lemmatize(w)\n",
        "    # print(w)\n",
        "    # w = w.lower()\n",
        "    yelp_negatives[idx] = (yelp_negatives[idx][0], w)\n",
        "for idx, d in enumerate(yelp_positives):\n",
        "    w = d[1]\n",
        "    # print(w)\n",
        "    w = remove_punctuation(w)\n",
        "    # print(w)\n",
        "    w = remove_stop_words(w)\n",
        "    # print(w)\n",
        "    w = lemmatize(w)\n",
        "    # print(w)\n",
        "    # w = w.lower()\n",
        "    yelp_positives[idx] = (yelp_positives[idx][0], w)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOG3jXDj8o9R",
        "colab_type": "text"
      },
      "source": [
        "# C: Separate Training and Testing Data\n",
        "In this assignment, for each file, please use the first 800 instances for each label as the training set and the remaining 200 instances as testing set. In\n",
        "total, there are 2400 reviews for training and 600 reviews for testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sB-2RQAy8sIY",
        "colab_type": "code",
        "outputId": "2fd4bbe6-b81f-4fe6-adaf-2567de4bf8fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "train = amazon_positives[:400] + amazon_negatives[:400] + imdb_positives[:400] + imdb_negatives[:400] + yelp_positives[:400] + yelp_negatives[:400]\n",
        "print(len(train))\n",
        "test = amazon_positives[400:500] + amazon_negatives[400:500] + imdb_positives[400:500] + imdb_negatives[400:500] + yelp_positives[400:500] + yelp_negatives[400:500]\n",
        "print(len(test))\n",
        "\n",
        "# train = positives[:400] + negatives[:400]\n",
        "# test = positives[400:500] + negatives[400:500]\n",
        "X_train = [x[1] for x in train]\n",
        "Y_train = [int(x[0]) for x in train]\n",
        "X_test = [x[1] for x in test]\n",
        "Y_test = [int(x[0]) for x in test]\n",
        "print(X_train[0])\n",
        "print(Y_train[0])\n",
        "print(X_test[0])\n",
        "print(Y_test[0])"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2400\n",
            "600\n",
            "good case excel valu\n",
            "1\n",
            "this great deal\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCHTBFBhCeu3",
        "colab_type": "text"
      },
      "source": [
        "# D: Bag of words model\n",
        "Extract features and then represent each review using bag of words\n",
        "model, i.e., every word in the review becomes its own element in a feature vector. In order to\n",
        "do this, first, make one pass through all the reviews in the training set (Explain why we can’t\n",
        "use testing set at this point) and build a dictionary of unique words. Then, make another pass\n",
        "through the review in both the training set and testing set and count up the occurrences of\n",
        "each word in your dictionary. The ith element of a review’s feature vector is the number of\n",
        "occurrences of the ith dictionary word in the review. Implement the bag of words model and\n",
        "report feature vectors of any two reviews in the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69FX1VoCCtX-",
        "colab_type": "code",
        "outputId": "04137720-7dd3-4489-a38b-eb3c5aa0a497",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "# Extract features and represent as bag of words model\n",
        "# TODO: Make this into a function to use for N-Gram part \n",
        "\n",
        "# Dictionary of unique words in our training set\n",
        "all_words = {}\n",
        "for r in X_train:\n",
        "    # Split up words in this review\n",
        "    words = r.split(' ')\n",
        "    # Initialize feature vector dict with just the words and 0 count\n",
        "    for w in words:\n",
        "        if w not in all_words:\n",
        "            all_words[w] = 0\n",
        "\n",
        "unique_words_list = list(all_words.keys())\n",
        "print(\"Unique words({}): {}\".format(len(unique_words_list), unique_words_list))\n",
        "\n",
        "test = [\"this phone is a good phone\"]\n",
        "# Create a feature matrix of all review's bag of words feature vectors\n",
        "feature_matrix = []\n",
        "all_reviews = X_train + X_test\n",
        "# all_reviews = test\n",
        "for r in all_reviews:\n",
        "    # print(\"Looking at review: {}\".format(r))\n",
        "    split_review = r.split(' ')\n",
        "    feature_vector = [0] * len(unique_words_list)\n",
        "    for s in split_review:\n",
        "        if s in unique_words_list:\n",
        "            idx = unique_words_list.index(s)\n",
        "            feature_vector[idx] += 1\n",
        "    feature_matrix.append(feature_vector)\n",
        "\n",
        "print(len(feature_matrix))\n",
        "print(all_reviews[-1])\n",
        "print(feature_matrix[-1])\n",
        "print(all_reviews[17])\n",
        "print(feature_matrix[17])\n",
        "# print(feature_matrix[17][unique_words_list.index(\"situations:1.)\")])\n",
        "\n",
        "# Save a couple feature vectors to output file for submission\n",
        "# np.savetxt(\"feature_vectors.csv\", y_pred, delimiter(\",\"))\n",
        "values = [17, -1]\n",
        "with open('feature_vectors.txt', 'w') as f:\n",
        "    for i in values:\n",
        "        f.write(\"{}\\n\".format(all_reviews[i]))\n",
        "        print(\"Writing sentence: {}\".format(all_reviews[i]))\n",
        "        for idx, item in enumerate(feature_matrix[i]):\n",
        "            f.write(\"{}, \".format(item))\n",
        "        f.write(\"\\n\")"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique words(3644): ['good', 'case', 'excel', 'valu', 'great', 'jawbon', 'the', 'mic', 'if', 'razr', 'owner', 'must', 'and', 'sound', 'qualiti', 'he', 'impress', 'go', 'origin', 'batteri', 'extend', 'veri', 'though', 'high', 'recommend', 'one', 'blue', 'tooth', 'phone', 'so', 'far', 'work', 'i', 'bought', 'use', 'kindl', 'fire', 'absolut', 'love', 'yet', 'run', 'new', 'two', 'bar', \"'s\", 'three', 'day', 'without', 'charg', 'pocket', 'pc', '/', 'combin', 've', 'own', '7', 'month', 'say', 'best', 'mobil', 'this', 'product', 'ideal', 'peopl', 'like', 'whose', 'ear', 'sensit', 'car', 'charger', 'well', 'ac', 'includ', 'make', 'sure', 'never', 'juic', 'highi', 'it', 'kept', 'fine', '680', 'camera', 'that', '2mp', 'pic', 'nice', 'clear', 'pictur', 'headset', 'price', 'right', 'bluetooth', 'featur', 'want', 'seem', 'made', 'protect', 'bulki', 'a', 'usabl', 'keyboard', 'actual', 'turn', 'pda', 'real-world', 'machin', 'instead', 'neat', 'gadget', 'pretti', 'sturdi', 'larg', 'problem', 'thing', 'everyth', 'reason', 'e', 'even', 'drop', 'stream', 'submerg', '15', 'second', 'still', 'happi', '510', 'no', 'complaint', 'regard', 'end', 'realli', 'facepl', 'sinc', 'look', 'eleg', 'cool', 'these', 'headphon', 'find', '-', 'think', 'perhap', 'purchas', 'last', 'sever', 'year', 'serious', 'feel', 'comfort', 'wear', 'glass', 'get', 'way', 'sometim', 'ipod', 'devic', 'situations:1', ')', 'choic', 'dock', 'station', 'home', 'beauti', 'littl', 'item', 'handi', 'lot', 'everyday', 'hold', 'packag', 'arriv', 'time', 'intend', 'easi', 'everyon', 'better', 'verizon', 'boy', 'cheaper', 'load', 'super', 'will', 'order', 'found', 'tri', '2', '(', 'listen', 'eas', 'integr', 'seamless', 'motorola', 'definit', 'bargain', 'free', 'ship', 'pros', ':', '-good', 'also', 'style', 'black', 'white', '350', 'my', 'jabra350', 'recept', 'piec', 'fit', 'call', 'wife', 'ask', 'slim', 'light', 'display', 'geeki', 'sex', 'toast', 'rock', 'ooz', 'embed', 'sleek', 'stylish', 'leather', 'fast', 'compromis', 'full', 'qwerti', 'basic', 'cell', 'number', 'keypad', 'winner', 'u', 'setup', 'could', \"n't\", 'simpler', 'differ', 'earpiec', 'jabra', 'first', 'iam', 'pleas', 'job', 'done', 'audio', 'week', 'small', 'realiz', 'accompani', 'softwar', 'almost', 'brilliant', 'avoid', 'damag', 'stuff', 'peachy-keen', 'voic', 'recognit', 'tremend', 'got', 'reccomend', 'relat', \"'m\", 'glad', 'link', '8530', 'blackberri', 'curv', 'know', 'funni', 'sketchi', 'technolog', 'would', 'send', 'kind', 'messag', 'web', 'brows', 'signific', 'faster', 'previous', 'build', 'unlik', 'cheap', 's***', 'fantast', 'perfect', 'color', 'w810i', 'superb', 'charm', 'sharp', 'screen', 'graphic', 'servic', 'igo', 'tip', 'file', 'browser', 'offer', 'option', 'need', 'handsfre', 'network', 'connect', 'hs850', 'whether', 'function', 'awesom', 'incred', 'ani', 'ring', 'tone', 'overal', 'buy', 'late', 'extrem', 'help', 'weight', 'hard', 'notic', 'you', 'll', 'thin', 'prettier', 'thought', 'all', 'invest', 'electron', 'avail', 'fm', 'transmitt', 'h500', '``', '1', \"''\", 'mega', 'pixel', 'part', 'good7', 'near', 'transmit', 'rang', 'decent', 'abl', 'roam', 'around', 'hous', 'live', 'room', 'reception/sound', 'issu', 'infatu', 'simpl', 'lightweight', 'certain', 'usual', 'headband', 'mess', 'hair', 'favorit', 'ever', 'market', 'authent', 'shine', 'excit', 'cute', 'worth', 'everi', 'penni', 'wallet', 'type', 'probabl', 'import', 'aspect', 'glove', 'strong', 'secur', 'durabl', 'o', 'gosh', 'attract', 'appear', 'form', 'factor', 'hand', 'tool', 'stereo', 'onli', 'flawless', 'life', 'real', 'gave', '5', 'star', 'revers', 'plug', 'rotat', 'whole', 'famili', 'seller', 'plantron', 'ador', 'instal', 'let', 'overnit', 'thru', 'handset', 'cat', 'attack', 'scratch', 'strip', 'destroy', 'now', 'wise', 'decis', 'someon', 'shouldv', 'invent', 'sooner', 'engin', 'clever', 'construct', 'receiv', 'quick', 'has', 'enough', 'alarm', 'clock', 'remov', 'we', 'gotten', 'compliment', 'state', 'allow', 'usag', 'drive', 'dial', 'low', 'replac', 'came', 'ago', 'either', 'loud', 'coupl', 'button', 'concret', 'knock', 'wood', 'transform', 'organiz', 'capabl', 'easier', 'advertis', 'microphon', 'accord', 'applifi', 'design', 'transmiss', 's11', 'data', 'cabl', 'finish', 'long', 'happier', 'side', 'skype', 'take', 'self', 'portrait', 'outsid', 'exterior', 'after', 'mani', 'final', 'magic', 'compani', 'prompt', 'exact', 'deal', 'satisfi', 'encourag', 'give', \"'d\", 'expect', 'effect', 'especi', 'era', 'an', 'for', 'fall', '2000', 'iriv', 'spinn', 'psych', 'appoint', 'contact', 'sanyo', 'surviv', 'dozen', 'blacktop', 'ill', 'earphon', 'away', 'enter', 'modest', 'cellular', 'but', 'clariti', 'awsom', 'restor', 'perform', 'jx-10', 'seri', 'moto', 'q', 'figur', 'search', 'internet', 'size', 'big', 'key', 'pad', 'lit', 'amazon', 'portabl', 'colleagu', 'expens', 'shipment', 'solid', 'surefir', 'gx2', 'other', 'tmobil', '3', 'deliveri', 'long-wear', 'condit', 'worthwhil', 'overnight', 'wo', 'regret', 'hybrid', 'palmtop/camera/cellphon', 'role', 'bt250v', 'describ', 'im', 'bose', 'nois', 'cancel', 'amaz', 'nyc', 'commut', 'frog', 'eye', 'catch', 'talk', 'aluminum', 'palm', 'vx', '--', 'handheld', 'within', 'timefram', 'sourc', 'waterproof', 'thank', 'slide', 'edg', 'pant', 'back', 'store', 'nokia', 'shield', 'incredi', 'keep', 'left', 'trunk', 'carri', 'convers', 'hitch', 'practic', 'ampl', 'storag', 'place', 'eargel', 'channel', 'direct', 'increas', 'volum', 'shift', 'bubbl', 'peel', 'noth', 'droid', 'earset', 'outgo', 'total', 'understand', 'patient', 'friend', 'enjoy', 'virgin', 'wireless', 'quit', 'le', 'fix', 'troubl', 'access', 'download', 'rington', 'descript', 'said', 'adapt', 'play', 'louder', 'speaker', 'lack', 'howev', 'effort', 'they', 'than', 'their', 'research', 'develop', 'divis', 'obvious', 're', 'those', 'walkman', 'clip', 'belt', 'deffinit', '50', 'cent', 'up-and-com', 'behe', '5020', '24', 'hour', 'pain', 'lg', 'fraction', 'pair', 'treo', '700w', 'usb', 'transceiv', 'beat', 'finger', 'normal', 'apart', 'haul', 'anyon', 'brand', 'phones/mp3', 'player', 'post', 'detail', 'comment', 'grey', 'red', 'not', 'exist', 'cds', 'surpris', 'much', 'review', 'fabul', 'bitpim', 'program', 'transfer', 'wind-resist', 'forev', 'over', 'oper', 'set', 'loop', 'tini', 'address', 'book', 'reboot', 'rate', 'purchase-', 'samsung', 'flipphon', 'well-design', 'doe', 'went', 'smooth', 'detach', '325', 'cellphon', 'worn-out', 'ringer', 'upbeat', 'crisp', 'smallest', 'provid', 'stay', 'video', 'becaus', 'occupi', 'background', 'distract', 'least', 'entir', 'ca', '*', 'come', 'shot', 'flash', 'sos', 'signal', 'mini-usb', 'open', 'outperform', 'china', 'v325i', 'gel', 'r', '18', 'anoth', 'power', 'via', 'wall', 'outlet', 'grip', 'prevent', 'slip', 'simpli', 'in', 'span', 'exclaim', 'whoa', 'tv', 'alway', 'cord', 'freedom', 'pass', 'mark', 'show', 'sign', '100', '%', 'soft', 'tight', 'cut', 'face', 'shape', 'copier', 'wait', 'someth', 't-mobil', 'custom', 'anywher', 'sold', 'classi', 'krussel', 'tracfonewebsit', 'user', 'toactiv', 'etc', '4', 'whatev', 'next', 'blueant', 'supertooth', 'hands-fre', 'metro', 'pcs', 'sch-r450', 'slider', 'soni', 'premium', 'plenti', 'capac', 'confort', 'somewhat', 'period', 'ant', 'hey', 'was', 'pleasant', 'supris', 'given', 'cost', 'conveni', 'ride', 'smoother', 'highest', 'anti-glar', 'protector', 'date', 'just', 'general', 'inconspicu', 'boot', 'unless', 'convert', 'tie', '45', 'minut', 'major', 'jiggl', 'line', 'hundr', 'imagin', 'fun', 'needl', 'wast', 'money', 'seper', 'mere', '5+', 'ft', 'start', 'excess', 'static', 'garbl', 'odd', 'advis', 'fool', 'click', 'wonder', 'mechan', 'websit', 'follow', 'commerci', 'mislead', 'mother', 'instruct', 'couldnt', 'hear', 'pull', 'breakag', 'unaccept', 'unus', 'move', 'freeway', 'speed', 'contract', 'hate', 'min', 'short', 'poor', 'worthless', 'garbag', 'mind', 'gon', 'na', 'argu', 'return', 'disappoint', 'bad', 'essenti', 'forget', 'microsoft', 'tech', 'support', 'particular', 'angl', 'parti', 'drawback', 'mp3', 'front', 'cover', 'paus', 'skip', 'song', 'lock', 'later', 'activ', 'sudden', 'die', 'bmw', 'fair', 'quiet', 'person', 'd807', 'wrong', 'longer', 'broke', '6', 'greater', 'bud', 'music', 'dont', 'plan', 'waaay', 'buyer', 'bewar', 'flush', 'toilet', 'suppos', '375', 'appar', 'match', 'huge', 'flaw', 'correct', 'although', 'megapixel', 'render', 'imag', 'resolut', 'purcash', 'may', 'complet', 'unhappi', 'bt', 'disapoin', 'care', 'logitech', 'earbud', 'fail', 'coverag', 'upstair', 'basement', 'experienc', 'area', 'liter', 'hope', 'wire', 'whine', 'communic', 'maintain', 'monkey', 'share', 'dna', 'copi', 'human', 'bougth', 'l7c', 'mode', 'put', 'latest', 'v1', '15g', 'slow', 'crawl', 'recogn', 'buzz', 'overrid', 'bluetoooth', 'thorn', 'abhor', 'recent', '10', 'disconnect', '13', 'buck', 'check', 'mail', 'night', 'backlight', 'lost', 'wit', 'hit', 'pleather', 'useless', 'deaf', 'strang', 'tick', 'bother', 'dollar', 'learn', 'lesson', 'onlin', 'anyway', 'earbug', 'mean', 'felt', 'crack', 'worst', 'freez', 'frequently4', 'embarrass', 'most', 'child-lik', 'consum', 'experi', 'horribl', 'bit', 'tell', 'ericsson', 'mistak', 'calendar', 'sync', 'addit', 'whatsoev', 'defeat', 'purpos', 'excruti', 'rubber/petroleum', 'smell', 'unbear', 'caus', 'flimsi', 'scari', 'absolutel', 'junk', '8', 'drain', 'potenti', 'fri', 'unreli', 'contstruct', 'hing', 'terribl', 'razor', 'v3i', 'complain', 'weak', '5-year', 'old', '2160', 'tracfon', 'manual', 'antena', 'uncomfort', 'compar', 'immedi', 'ngage', '2-3', 'cant', 'anyth', 'however-th', 'riington', 'neither', 'game', 'suck', 'rip', 'recharg', 'frequentyli', 'flip', 'adhes', 'inexpens', 'add', 'boost', 'sit', 'vehicl', 'cradl', 'jerk', 'los', 'angel', 'starter', 'loudspeak', 'bumper', 'appeal', 'improv', 'leak', 'hot', 'save', 'special', 'drivng', 'along', 'auto', 'tape', 'embarass', 'hurt', 'push', 'averag', 'soyo', 'mention', 'crap', 'comparably-pr', 'today', 'reciev', 'stupid', 'kit', 'cingulair', 'nicer', 'dead', 'hoursth', 'thereplac', 'cheapli', 'att', 'distort', 'yell', 'plastic', 'break', 'oh', 'forgot', 'weird', 'unit', 'fond', 'magnet', 'strap', 'note', 'rather', 'bland', 'model', 'warn', 'wish', '1/2', 'earpad', 'displeas', 'might', 'defect', 'risk', 'built', 'alon', 'difficult', 'fulli', 'bed', 'wi-fi', '20', 'morn', 'read', 'memori', 'card', 'hat', 'sunglass', 'bt50', 'comput', 'remors', 'accessoryon', 'inexcus', 'chang', 'carrier', 'updat', 'procedur', 'cumbersom', 'vx9900', 'env', 'switch', 'rocket', 'destin', 'unknown', 'bill', 'unfortun', 'abil', 'piti', 'respect', 'exchang', 'result', 'stuck', 'max', 'mute', '11', 'holster', 'photo', 'ad', 'earlier', 'happen', 'standard', 'ugli', 'refus', 'refund', 'accident', 'gentle-touch', 'touch', 'threw', 'window', 'took', 'inch', 'kitchen', 'counter', 'laugh', 'none', 'proper', 'miss', 'numer', 'zero', 'exercis', 'frustrat', 'wirefli', 'cingular/at', 'inform', 'aggrav', 'muddi', 'insert', 'glu', 'slid', 'plantroninc', 'continu', 'disapoint', 'fourth', 'bare', 'constant', 'joke', 'due', 'forc', 'stop', 'walk', 're-set', 'wipe', 'strength', 'menu', 'navig', 'recess', 'broken', 'smoke', 'sprint', 'posses', 'idea', 'trash', 'killer', 'cours', 'infuri', 'europ', 'asia', 'crappi', 'e715', 'seeen', 'interfac', 'decad', 'compet', 'steer', 'genuin', 'replacementr', 'pen', 'three-pack', 'buyit', 'plus', 'believ', 'steep', 'point', 'dissapoint', 'extra', 'discard', 'pay', 'guess', 'current', 'first-person', 'shooter', 'delay', 'accessori', 'manufactur', 'aw', 'muffl', 'tinni', 'incom', 'echo', 'told', 'warranti', 'produc', 'receipt', 'luck', 'linksi', 'refurb', 'snug', 'heavi', 'utter', 'promis', 'four', 'spring', 'latch', 'visor', 'tungsten', 'e2', 'studi', 'interest', 'sin', 'industri', 'track', 'somehow', 'upload', 'third', 'random', 'truli', 'accept', 'balanc', 'readi', 'prime', 'chines', 'forgeri', 'abound', 'explain', 'jack', 'ca-42', 'biggest', 'superfast', 'ergonom', 'theori', 'stand', 'except', 'cbr', 'mp3s', 'prefer', 'medium', 'startac', 'sim', '3o', 'crash', 'iphon', 'despit', 'multipl', 'imac', 'extern', 'elsewher', 'bell', 'whistl', 'mediocr', 'sent', 'pro', 'texa', 'dit', '5320', 'main', 'soon', 'dustpan', 'indoor', 'dispos', 'puff', 'nano', 'son', 'reccommend', 'smartphon', 'wont', 'atleast', 'amp', 'reoccur', 'bottom', 'somewher', 'els', 'cingular', '12', 'creak', 'wooden', 'floor', 'slowli', 'sorri', 'imposs', 'upgrad', 'discount', 'possibl', 'doubl', 'disgust', 'rare', 'instanc', 'shout', 'telephon', 'wind', 'grtting', '44', 'v3c', 'improp', 'everywher', 'awkward', 'scene', 'movi', 'gerardo', 'head', 'saw', 'kid', 'cast', 'jimmi', 'buffet', 'scienc', 'teacher', 'babi', 'owl', 'florida', 'were', 'muppet', 'hilari', 'deliv', 'overdu', 'consid', 'tale', 'sister', 'singl', 'greatest', 'film', 'gem', 'term', 'screenplay', 'cinematographi', 'act', 'post-product', 'edit', 'film-mak', '\\x96', 'true', 'masterpiec', 'sea', 'faux', 'structur', 'easili', 'histori', 'cinema', 'vital', 'occur', 'word', 'content', 'level', 'fill', 'how', 'superl', 'yes', 'requir', 'amount', 'puzzle-solv', 'togeth', 'creat', 'th', 'deserv', 'insan', 'there', 'massiv', 'unlock', 'charact', 'as', 'canada', 'aye', 'pure', 'brillianc', 'succeed', 'meagr', 'budget', 'lesser', 'written', 'french', 'cancan', 'boast', 'cutest', 'lead', 'ladi', 'grace', 'head-over-heel', 'girl', 'stori', 'ann', 'hech', 'convinc', 'sam', 'shepard', 'portray', 'gung', 'ho', 'marin', 'sober', 'sat', 'rivet', 'resound', '9', 'tom', 'hank', 'actor', 'child', 'suspens', 'builder', 'cross', 'g', 'pg', 'non-clich', 'parent', 'predict', 'dialog', 'verbatim', 'write', 'select', 'gross', 'chill', 'alexand', 'nevski', 'artist', 'whoever', 'word-of-mouth', 'promot', 'thought-provok', 'well-pac', 'suit', 'lion', 'superbl', 'classic', 'b-list', 'horror/suspens', 'manna', 'from', 'heaven', 'terrif', 'unpredict', 'often', 'occasion', 'evalu', 'veteran', 'nostalgia', 'trip', 'ursula', 'burton', 'nun', 'church', 'shirley', 'jone', 'rendit', 'tonight', 'uplift', 'watch', 'sceneri', 'fresh', 'bold', 'mayb', 'idiot-sav', 'joy', 'some', 'applaus', 'prelud', 'director', 'seat', 'afraid', 'see', '10/10', 'world', 'social', 'physic', 'cult', 'view', 'treat', 'anthoni', 'quinn', 'crazi', 'hors', 'empower', 'woman', 'ms', 'garbo', 'bat', 'talent', 'silent', 'netflix', 'stock', 'renown', 'screenwrit', 'franc', 'marion', 'step', 'age', 'john', 'wayn', 'young', 'his', 'presenc', 'senior', 'older', 'jami', 'foxx', 'is', 'ray', 'charl', 'genius', 'spacek', 'coal', 'miner', 'daughter', 'quaid', 'ball', 'fact', 'rememb', 'man', 'legendari', \"'\", 'biograph', 'materi', 'beyond', 'musician', 'hitchcock', 'thriller', 'chase', 'when', '80', 'fascin', 'danc', 'intent', 'master', 'theme', 'would-b', 'underton', 'fifti', 'existenti', 'world-weari', 'aerial', 'ought', 'thrill', 'sen', 'deepli', '***spoilers***', 'surfac', 'craft', 'stun', 'fx', 'state-of-the-art', 'conceptu', 'everything-', 'everybodi', 'fantasi', 'and/or', 'fan', 'sour', 'struck', 'acting-wis', 'releas', 'mexican', 'understood', 'matter', 'identifi', 'rank', 'noir-crime-drama', 'belmondo', 'lino', 'ventura', 'attent', 'complex', 'psycholog', 'loyalti', 'treacheri', 'drama', 'up', 'melvill', 'journey', 'soul', 'water', 'manag', 'transcend', 'limit', 'indi', 'flick', 'subvert', 'emerg', 'intens', 'crocdodil', 'inde', 'croc', 'swamp', 'locat', 'thorough', 'christoph', 'eccleston', 'control', 'tardi', 'trilog', 'kieslowski', 'ceas', 'favourit', 'colour', 'flag', 'art', 'visual', 'seen', 'subtl', 'spoiler', 'remain', 'survivor', 'ferri', 'disast', 'valentin', 'judg', 'solidifi', 'suffer', 'dealt', 'smile', 'wrap', 'roth', 'pearl', 'award', 'eloqu', 'franci', 'ford', 'coppola', '25', 'unfold', 'gradual', 'leaf', 'lie', 'contrast', 'nicola', 'roeg', 'wih', 'sublim', 'entertain', 'outlandish', 'array', 'memor', 'psychot', 'lovabl', 'nut', 'danger', 'sweet', 'moment', 'repair', 'reaction', 'bitchi', 'bos', 'smart', 'twist', 'shed', 'tear', 'educ', 'barney', 'de', 'duper', 'bop', 'televis', 'writer', 'smack', 'actress', 'bonus', 'episod', '10+', 'endear', 'becom', 'wholesom', 'sibl', 'bond', 'action', 'unneed', 'controversi', 'damian', 'versatil', 'modern', 'passion', 'audienc', 'elia', 'kotea', 'palanc', 'angelina', 'nake', 'billi', 'drago', '+', 'cameo', 'sven', 'ole', 'thorsen', 'ebay', 'poler', 'bear', 'kinda', 'question', 'fort', 'steel', 'excerpt', 'styliz', 'exemplar', 'explor', 'natur', 'poetri', 'theater', 'polit', 'japanes', 'here', 'america', 'imperi', 'what', 'faultless', 'photographi', 'composit', 'underappreci', 'brian', 'keith', 'bulli', 'teddi', 'vivid', 'member', 'sean', 'conneri', 'nobl', 'brigand', 'candac', 'bergen', 'feisti', 'heroin', 'huston', 'wili', 'hay', 'steve', 'kanali', 'spiffi', 'radiant', 'ruthless', 'can-do', 'lieuten', 'roosevelt', 'stick', 'adventur', 'robert', 'ryan', 'father', 'schizophren', 'murder', 'affect', 'war', 'wors', 'have', 'humour', 'apt', 'brother', 'imit', 'individu', 'non-linear', 'narrat', 'thus', 'flashback', 'articul', 'monica', 'bellucci', 'commentari', 'undoubt', 'timer', 'popular', 'feel-good', 'northern', 'posit', 'communiti', 'repres', 'uniqu', 'vivian', 'schill', 'script', '95', 'theatr', 'her', 'screami', 'masculin', 'european', 'throwback', 'student', '1980', 'abroad', 'interact', 'nation', 'circumst', 'slight', 'scamp', 'yelp', 'scare', 'funniest', 'caught', 'curtain', 'sing', 'ive', 'had', 'befor', 'june', 'underneath', 'lid', 'romant', 'junkyard', 'dog', 'too', 'premis', 'nut-bag', 'stephen', 'mchatti', 'lanc', 'hendrikson', 'rage', 'cheekbon', 'soundtrack', 'oy-vey', 'scale', 'both', 'amus', 'armand', 'assant', 'summari', 'twice', 'alreadi', 'loos', '8/10', 'score', 'plot', 'knew', 'gift', 'jaclyn', 'smith', 'god', 'contain', 'all-star', 'length', '1-10', 'lifetim', 'air', 'sell', 'must-hav', 'understat', 'comedi', 'univers', 'team', 'behind', 'trond', 'fausa', 'aurvåg', 'bothersom', 'humor', '2006', 'initi', 'local', 'site', 'buffalo', 'intrigu', 'applaud', 'conclus', 'lilt', 'heart', 'race', 'duri', 'rest', 'noteworthi', 'barcelona', 'fame', 'gaudi', 'tower', 'martin', 'middle-ag', 'upper', 'class', 'uptight', 'guy', 'betti', 'jean', 'steamboat', 'willi', 'mickey', 'mous', 'plane', 'famous', 'ground-break', 'while', 'speak', 'ton', 'throughout', 'grant', 'crowd', 'pleaser', '1928', 'timeless', 'turkey', 'straw', 'cruel', 'crowd-pleas', 'among', 'bodi', '1947', 'garfield', 'rever', 'lilli', 'plmer', 'william', 'conrad', 'lee', 'cinematograph', 'jame', 'wong', 'howe', 'okay', 'critic', 'credit', 'creatur', 'ta', 'close-up', 'slimi', 'drool', 'teeth', 'sole', 'bright', 'spot', 'jonah', 'hill', 'unrecogniz', 'superbad', 'interim', 'be', '90', 'proud', 'wb', 'cartoon', 'toon', 'vibe', 'underr', 'murki', 'tender', 'dark', 'sitcom', 'orient', 'teenag', 'peculiar', 'perabo', 'energi', 'pitch', 'instant', 'catchi', 'do', 'simmer', 'boil', 'wart', 'poet', 'bohemian', 'self-indulg', 'wartim', 'bombard', 'london', 'outward', 'tranquil', 'welsh', 'coastal', 'retreat', 'borderlin', 'friendship', 'lust', 'dedic', 'versus', 'concern', 'jealousi', 'rivalri', 'cowardic', 'egot', 'heroism', 'self-sacrific', 'matur', 'suggest', 'bring', 'dramat', 'focus', 'under', 'tension', 'serv', 'inappropri', 'keira', 'knightley', 'prone', 'exquisit', 'pointillist', 'footag', 'behold', 'bertolucci', 'relationship', 'engag', 'christma', 'errol', 'flynn', 'custer', 'alongsid', 'olivia', 'havilland', 'known', 'dad', 'grew', 'realis', 'shenanigan', 'surround', 'spi', 'kidnap', 'nasti', 'foreign', 'taxidermist', 'stewart', 'hero', 'climax', 'embassi', 'brood', 'menac', 'low-budget', 'nevertheless', 'stabl', 'top', 'typic', 'sci', 'fi', 'sci-fi', 'scot', 'vandiv', 'nonetheless', 'campi', 'sort', 'bore', 'delight', 'reveal', 'task', 'challeng', 'south', 'africa', 'past', 'attempt', 'truth', 'reconcili', 'process', 'magnific', 'photography/cinematographi', 'woven', 'splendid', 'rent', 'subtitl', 'avers', 'therapi', 'shakespear', 'macbeth', 'jason', 'monolog', 'brief', 'candl', 'sphere', 'moral', 'decay', 'helen', 'baxendal', 'credibl', 'cheerful', 'naughti', 'tast', 'blood', 'evil', 'death', 'lyric', 'witti', 'dr', 'seuss', 'anim', 'upa', 'finest', 'academi', 'element', 'paul', 'schrader', 'mishima', 'notabl', 'bailey', 'costum', 'eiko', 'ishioka', 'dracula', 'oscar', 'consol', 'raw', 'intellig', 'stage', 'theatric', 'terror', 'escal', 'monstrous', 'consequ', 'mesmeris', 'wilkinson', 'fumbl', 'hanki', 'male', 'femal', 'alik', 'julian', 'fellow', 'triumph', 'treasur', 'well-don', 'phrase', 'conflict', 'owe', 'kris', 'kristoffersen', 'documentari', 'marriag', 'punish', 'park', 'southern', 'california', 'desert', 'patent', 'watkin', 'peter', 'a+', 'stanwyck', 'morgan', 'equival', 'dicken', 'carol', 'sensibl', 'success', 'depend', 'sydney', 'greenstreet', 'yardley', 'eye-pleas', 'relax', 'henc', 'titl', 'handl', 'tough', 'digniti', 'shock', 'whenev', 'emili', 'watson', 'uncondit', 'prepar', 'ordeal', 'begin', 'progress', 'anguish', 'glanc', 'pan', 'interview', 'with', 'vampir', 'lestat', 'stuart', 'townsend', 'cruis', 'aailiyah', 'akasha', 'compel', 'entranc', 'mini', 'rice', 'treatment', 'dysfunct', 'skill', 'meredith', 'm', 'sentiment', 'worthi', 'trap', 'syrupi', 'indulg', 'nine', 'ten', 'earli', 'futur', 'goremeist', 'lucio', 'fulci', 'giallo', 'sub-genr', 'italian', '70', 'standout', 'technic', 'camera-work', 'riz', 'ortolani', 'recur', 'unaccompani', 'vocal', 'distant', 'wild', 'dustin', 'hoffman', 'involv', 'ps', 'central', 'blown', 'jennif', 'rubin', 'harri', 'nervous', 'starlet', 'event', 'insid', 'emot', 'busi', 'situat', 'coach', 'snow', 'heard', '1973', 'stranger', 'duet', 'astronaut', 'doctor', 'cold', 'fear', 'ussr', 'scream', 'horror', 'extraordinari', 'courtroom', 'indict', 'american', 'justic', 'system', 'frighten', 'highlight', 'fundament', 'legal', 'discov', 'guilt', 'innoc', 'present', 'court', 'pace', 'underst', 'routin', 'fact-bas', 'cole', 'she', 'constantin', 'motiv', 'inspir', 'dvd', 'phenomen', 'opinion', 'subject', 'jay', 'adam', 'neil', 'evok', 'of', 'graini', 'enhanc', 'achill', 'philippa', 'don', 'giovanni', 'appreci', 'layer', '18th', 'centuri', 'jutland', 'instrument', 'loewenhielm', 'partak', 'caill', 'en', 'sarcophag', 'sampl', 'savor', 'warmth', 'generat', 'auster', 'backdrop', 'leav', 'notch', 'flesh', 'grime', 'blake', 'sharpli', 'deadpan', 'tongu', 'cheek', 'dialogu', 'realist', 'angus', 'scrimm', 'gentl', 'violin-play', 'anatomist', 'slow-mov', 'aimless', 'distress', 'drift', 'flat', 'half', 'arti', 'becam', 'ridicul', 'non-exist', 'empti', 'punch', 'doom', 'concept', 'lame', 'minor', 'pg-13', 'non-sequel', 'pg-rat', 'confirm', 'unfunni', 'generic', 'exagger', 'trailer', 'carrel', 'co-star', 'fare', 'freeman', 'ed', 'helm', 'lazi', 'blue/green-screen', 'translat', 'on', 'negat', 'insipid', 'whini', 'pointless', 'annoy', 'fingernail', 'chalkboard', 'unnecessari', 'train/rol', 'coaster', 'grate', 'nerv', 'improvis', 'worri', 'honest', 'crackl', 'youth', 'concentr', 'meander', 'pretenti', 'dodg', 'stratus', 'maker', 'restrain', 'québec', 'horrend', 'retard', 'frank', 'cotton', 'club', 'unfaith', 'lane', 'gere', 'shallow', 'insincer', 'chick', 'lousi', 'peak', 'paper', 'columbo', 'oh-so-matur', 'neighbour-girl', 'misplac', 'weaker', 'then', 'debat', 'sack', 'trumpet', 'fals', 'accus', 'villain', 'michael', 'ironsid', 'chemistri', 'ben', 'affleck', 'sandra', 'bullock', 'wife-to-b', 'excruci', 'remak', 'wed', 'agre', 'lose', 'nobodi', 'dribbl', 'imdb', 'mirrormask', 'unsatisfactori', 'inexperi', 'meant', 'inept', 'stereotyp', 'depth', 'core', 'bunch', 'schooler', 'cri', 'hell', 'confus', 'fish', 'underwat', 'repeat', 'thousand', 'redeem', 'mst3k', 'fodder', 'paid', 'again', 'relief', 'yeah', 'or', 'storylin', 'pillow', 'girlfriend/boyfriend', 'gone', 'dislik', 'ticket', 'five', 'mad', 'cardboard', 'cutout', 'reverse-stereotyp', 'haggi', 'stroke', 'storytel', 'paint', 'crayon', 'depress', 'provok', 'teach', 'racism', 'prejudic', 'your', 'brain', 'shut-down', 'primal', 'impuls', 'self-preserv', 'shatter', 'recov', 'join', 'reveng', 'boogeyman', 'zombiez', 'hellish', 'triniti', 'distinct', 'appal', 'artless', 'endless', 'ugliest', 'charisma-fre', 'merit', 'akin', 'tortur', 'explan', 'atroc', 'voyag', 'self-discoveri', 'unrecommend', 'close', 'drag', 'spent', 'showcas', 'gallon', 'spew', 'forth', 'iron', 'pander', 'sabotag', 'rumbl', 'desper', 'logic', 'uninterest', 'chilli', 'unremark', 'author', 'living/work', 'abstrus', 'cultur', 'reenact', 'adrift', 'stagi', 'soldier', 'pledg', 'hairsplit', 'puriti', 'admir', 'sword', 'kill', 'momentum', 'quicker', 'resum', 'amateurish', 'hole', 'inconsist', 'horrid', 'ballet', 'repertori', 'pathet', 'woa', 'sappiest', 'unwatch', 'filmographi', 'chanc', 'regrett', 'express', 'celebr', 'patriot', 'underlin', 'allison', 'plain', 'cinematography-if', 'that-suck', 'concert', 'sequenc', 'angri', 'disturb', 'pi**', 'jerki', 'camerawork', 'sick', 'wittic', 'bob', 'rise', 'vomit', 'offens', 'strive', 'muddl', 'dispar', 'accur', 'defin', 'to', 'sens', 'toler', 'incorrect', 'disbelief', 'slavic', 'which', 'malta', 'dri', 'barren', 'hockey', 'defensemen', 'goali', 'dive', 'foot', 'wide', 'net', 'bakeri', 'assist', 'waitress', 'superfici', 'stagey', 'farc', 'by', 'pyromaniac', 'waylaid', 'did', 'yawn', 'storm', 'trooper', 'list', 'pot', 'plant', 'shame', 'ireland', 'horrifi', 'sympathet', 'toward', 'movement', 'shelf', 'unrealist', '70000', 'luci', 'higher', 'sink', 'slacker', 'excus', 'old-fashion', 'tune', 'rita', 'hayworth', 'pedest', 'exampl', 'hollywood', 'exploit', 'financi', 'gain', 'ticker', 'hummh', 'blew', 'chosen', 'blatant', 'propaganda', 'hatr', 'cliché', 'admit', 'school', 'pile', 'round', 'admin', 'thunderbird', 'slow-mot', 'needless', 'thrown', 'corn', 'flake', 'box', 'directori', 'border', 'filmmak', 'hide', 'sand', 'monument', 'halfway', 'howel', '1/10', '0/10', 'profession', 'debbi', 'rochon', 'contributori', 'former', 'chimp-lik', 'simplifi', 'sake', 'breviti', 'fulfil', 'convolut', 'girlfriend', 'bela', 'lugosi', 'extran', 'inton', 'decid', 'b', 'wow', 'comprehens', 'hang', 'camp', 'iq', 'mollusk', 'insult', 'movie-go', 'public', 'string', 'lower', 'fox', 'victor', 'mclaglen', 'donlevi', 'taylor', 'presid', 's', 'row', 'unmov', 'cheerless', 'heist', 'characteris', 'underbit', 'stoic', 'chow', 'yun', 'fat', 'tomorrow', 'cheesi', 'abandon', 'factori', 'execut', 'fli', 'judo', 'roll', 'la', 'woo', 'squib', 'at', 'proceed', 'remot', 'deriv', 'merci', 'hopeless', 'overact', 'space', 'uneasi', 'compos', 'about', '30', 'elder', 'babbl', 'overwrought', 'pseudo-satan', 'gibberish', 'corni', 'teen', 'goth', 'blush', 'english', 'latin', 'sloppi', 'hackney', 'blah', 'underact', 'jerri', 'falwel', 'assault', 'strident', 'blare', 'pap', 'afternoon', 'unemploy', 'despis', 'rough', 'draft', 'shoot', 'began', 'irrit', 'indescrib', 'idiot', 'season', 'uneth', 'nc-17', 'zillion', 'realiti', 'wash', 'twirl', 'grade', 'z', 'out', '20th', 'road', '1948', 'silli', 'noir', 'implaus', 'unmitig', 'unconvinc', 'edward', 'chodorov', 'negulesco', 'widmark', 'unintent', 'comic', 'marbl', 'mar', 'studio', 'racial', 'offend', 'litter', 'overt', 'slur', 'depict', 'moron', 'boob', 'asid', 'latifa', 'wouldnt', 'bipolar', 'thug', 'luv', 'diaper', 'accolad', 'thomerson', '1986', 'version', 'watchabl', 'problems\\x97th', 'professor', 'dumb', 'establish', 'zombie-stud', 'necklac', 'meteorit', 'matthew', 'wrote/direct', '1995', 'monster', 'grim', 'gore', 'convent', 'cg', 'slideshow', 'explos', 'gas', 'tank', 'sum', 'breeder', 'ebola', 'virus', 'charisma', 'comed', 'debit', 'popcorn', 'lewi', 'consider', 'incendiari', 'unrestrain', 'evid', 'reflect', 'below-par', 'borrow', 'filmi', 'expans', 'convey', 'broad', 'sweep', 'landscap', 'asleep', 'trashi', '-period', 'sequel', 'precis', 'phoni', 'contriv', 'fifteen', 'um', 'volcano', 'unbeliev', 'nonsens', 'foolish', 'accent', 'abysm', 'hypocrisi', 'rubbish', 'non-research', 'omit', 'sidelin', 'inexplic', 'crew', 'regardless', 'jim', \"o'connor\", 'energet', 'georg', 'dull', 'host', 'voice-ov', 'monoton', 'guest', 'mind-bend', 'medic', 'terminolog', 'iffi', 'insulin', 'diabet', '1971', 'format', 'mini-seri', 'baaaaaad', 'collect', 'extant', 'austen', 'kitchi', 'error', 'garag', 'joe', 'facial', 'configur', 'overcom', 'reject', 'miser', 'hollow', 'laughabl', 'angela', 'bennett', 'expert', 'pretext', 'robot', 'delet', 'uncal', 'geek', 'bibl', 'thumper', 'bad-ass', 'mouth', 'rpg', 'disgrac', 'self-respect', 'rpger', 'youtub', 'lassi', 'sleep', 'dreari', 'time-wast', 'forgett', 'achiev', \"so-bad-it's-good\", \"so-bad-it's-memor\", 'shell', 'bark', 'bank', 'holiday', 'rick', 'cape', 'cod', 'ravoli', 'chicken', 'cranberri', 'mmmm', 'food', 'interior', 'velvet', 'cake', 'ohhh', 'street', 'taco', 'staff', 'combo', 'burger', 'beer', '23', 'accid', 'restaur', 'portion', 'visit', 'hiro', 'shrimp', 'moist', 'server', 'rib', 'dessert', 'section', 'beef', 'cook', 'sandwich', 'firehous', 'greek', 'salad', 'dress', 'tasti', 'pita', 'hummus', 'refresh', 'duck', 'pink', 'char', 'husband', 'tabl', 'mein', 'sauc', 'scallop', 'appet', 'cocktail', 'handmad', 'delici', 'militari', 'dos', 'gringo', 'drink', 'jeff', 'potato', 'lunch', 'pho', 'yummi', 'omelet', 'sexi', 'outrag', 'flirt', 'hottest', 'breakfast', 'cafe', 'serf', 'our', 'roast', 'garlic', 'bone', 'marrow', 'meal', 'waiter', 'bloddi', 'mari', 'town', 'mussel', 'wine', 'reduct', 'dish', 'tigerlilli', 'bartend', 'ambienc', 'sooooo', 'thai', 'spici', 'steak', 'eaten', 'ventur', 'sushi', 'ambianc', 'pork', 'belli', 'vega', 'penn', 'vodka', 'meatloaf', 'crispi', 'delish', 'tuna', 'melt', 'bagel', 'cream', 'chees', 'lox', 'caper', 'subway', 'meet', 'blanket', 'moz', 'subpar', 'bathroom', 'clean', 'decor', 'fiancé', 'middl', 'greet', 'mandalay', 'bay', 'nigiri', 'joint', 'flavor', 'voodoo', 'pasta', 'gluten', 'divers', 'hands-down', 'eat', 'phoenix', 'bacon', 'hella', 'salti', 'dine', 'lordi', 'khao', 'soi', 'curri', 'lover', 'accommod', 'vegetarian', 'atmospher', 'hip', 'cozi', 'haunt', 'healthi', 'quantiti', 'lemon', 'raspberri', 'ice', 'crepe', 'bread', 'butter', 'chip', 'egg', 'wing', 'bowl', 'exceed', 'dream', 'serivc', 'brunch', 'invit', 'mix', 'mushroom', 'yukon', 'gold', 'beateous', 'salmon', 'tartar', 'jamaican', 'mojito', 'rich', 'folk', 'otto', 'welcom', 'must-stop', 'chef', 'sport', 'hardest', 'flavour', 'ayc', 'mood', 'creami', 'pizza', 'peanut', 'seafood', 'generous', 'veggitarian', 'platter', 'madison', 'ironman', 'patio', 'outstand', 'goat', 'skimp', 'meat', 'bachi', 'soooo', 'service-check', 'summer', 'outdoor', 'bbq', 'lighter', 'in-hous', 'shawarrrrrrma', 'pea', 'unreal', 'vinaigrett', 'omg', 'delicioso', 'veget', 'tucson', 'chipotl', 'classy/warm', 'succul', 'basebal', 'brick', 'oven', 'app', 'equal', 'enthusiast', 'door', 'rowdi', 'crave', 'eggplant', 'green', 'bean', 'stir', 'dinner', 'outshin', 'halibut', 'def', 'ethic', 'andddd', 'stuf', 'crystal', 'shop', 'mall', 'aria', 'summar', 'nay', 'pneumat', 'condiment', 'dispens', 'kiddo', 'impecc', 'remind', 'mom', 'pop', 'san', 'francisco', 'buldogi', 'gourmet', 'petti', 'tea', 'hungri', 'assur', 'regular', 'overwhelm', 'companion', 'textur', 'expert/connisseur', 'topic', 'nicest', 'across', 'biscuit', 'absolutley', 'steiner', 'familiar', 'fs', 'breakfast/lunch', 'each', 'pear', 'almond', 'spicier', 'ribey', 'mesquit', 'gooodd', 'venu', 'group', 'nargil', 'tater', 'tot', 'southwest', 'vanilla', 'profiterol', 'chou', 'pastri', 'az', 'margarita', 'trim', '70+', 'claim', '40', 'jewel', 'las', 'toro', 'cavier', 'slice', 'wagyu', 'truffl', 'daili', 'pancak', 'crawfish', 'multi-grain', 'pumpkin', 'pecan', 'fluffi', 'homemad', '/hand', 'cheesecurd', 'pack', 'to-go', 'tiramisu', 'cannoli', 'thumb', 'sun', 'choos', 'frenchman', 'entre', 'gc', 'opportun', 'yellowtail', 'carpaccio', 'boyfriend', 'donut', 'tapa', '*heart*', 'hanker', 'effici', 'perpar', 'giant', 'dust', 'powder', 'sugar', 'fo', 'accomod', 'vegan/veggi', 'crouton', 'crema', 'café', 'expand', 'philadelphia', 'north', 'scottsdal', 'cow', 'grill', 'yellow', 'saffron', 'shirt', 'high-qual', 'caesar', 'proven', 'macaron', 'sit-down', 'legit', 'somethat', 'duo', 'violinist', 'request', 'baklava', 'falafel', 'baba', 'ganoush', 'mgm', 'courteous', 'eclect', 'onion', 'nobu', 'similar', 'cheeseburg', 'neighborhood', 'panna', 'cotta', 'damn', 'slaw', 'drench', 'mayo', 'piano', 'rge', 'fillet', 'relleno', 'plate', 'sergeant', 'pepper', 'auju', 'hawaiian', 'breez', 'mango', 'pineappl', 'smoothi', '2007', 'cashier', 'brought', 'crust', 'honeslti', 'rubber', 'ahead', 'warmer', 'wayyy', 'overpr', 'indic', 'cash', 'burritto', 'luke', 'warm', 'sashimi', 'blow', 'grab', 'bite', 'pub', 'favor', 'styrofoam', 'frozen', 'puck', 'regist', 'attitud', 'downtown', 'flat-lin', 'banana', 'petrifi', 'tasteless', 'refil', 'struggl', 'wave', 'salt', 'batter', 'chewi', 'meh', 'milkshak', 'chocol', 'milk', 'excalibur', 'common', 'cheat', 'underwhelm', 'greas', 'rave', 'sugari', 'tailor', 'palat', 'six', 'ratio', 'unsatisfi', 'casino', 'forward', 'bye', 'yama', '40min', 'blandest', 'indian', 'cuisin', 'bitch', 'rude', 'weekend', 'ate', 'bamboo', 'ignor', 'forti', 'vain', 'crostini', 'stale', 'leftov', 'reloc', 'spinach', 'avocado', 'ingredi', 'sad', 'desir', 'overcook', 'charcoal', 'verg', 'dirt', 'gyro', 'joey', 'vote', 'valley', 'reader', 'magazin', 'friday', 'disrespect', '1979', 'bug', 'climb', 'soggi', 'shower', 'rins', 'nude', 'lobster', 'bisqu', 'bussel', 'sprout', 'risotto', 'filet', 'bode', 'uninspir', 'worst/annoy', 'drunk', 'patti', 'yum', 'eel', '-drink', 'base', 'sub-par', 'gratitud', 'privileg', 'working/', 'godfath', 'recal', 'tap', 'par', 'proclaim', 'coffe', 'boba', 'jenni', 'mac', 'stink', 'burn', 'saganaki', 'disagre', 'yelper', 'noodl', 'count', 'greedi', 'corpor', 'dime', 'atroci', 'muffin', 'untoast', 'bus', 'downsid', 'doubt', '00', 'honor', 'hut', 'coupon', 'dirty-', 'replenish', 'yucki', '17', 'spaghetti', 'gordon', 'ramsey', 'shall', 'despic', 'soup', 'lukewarm', 'stomach', 'ach', 'outta', 'inconsider', 'hi', 'starv', 'ian', 'bouchon', 'account', 'screw', 'sore', 'ground', 'smear', 'been-stepped-in-and-tracked-everywher', 'bird', 'poop', 'furthermor', 'strike', 'rush', '5lb', '3/4ths', 'gristl', 'anytim', 'connoisseur', 'driest', 'acknowledg', '35', 'ventil', 'letdown', 'camelback', 'flower', 'cartel', 'crab', 'leg', 'attach', 'humili', 'worker', 'name', 'tragedi', 'airlin', 'noca', 'lettuc', 'greasi', 'unhealthi', 'apolog', 'martini', 'thirti', 'vacant', 'hello', 'vinegrett', 'are', 'theft', 'eew', 'overhaul', 'swung', 'sucker', 'oliv', 'crumbi', 'pale', 'soooooo', 'freak', 'reheat', 'ok', 'wedg', 'bloodi', 'non-custom', 'grandmoth', 'hostess', 'same', 'drastic', 'madhous', 'edibl', 'plater', 'googl', 'smashburg', 'plantain', 'spend', 'mortifi', 'drip', 'hospit', 'paradis', 'refrain', 'cibo', 'elk', 'appl', 'edinburgh', 'revisit', 'stood', 'inflat', 'smaller', 'grow', 'rapid', 'mid', 'arepa', 'shoe', 'tradit', 'hunan', 'nutshel', 'restaraunt', 'dirti', 'sewer', 'veal', 'flavorless', 'tepid', 'chain', 'onc', '85', 'wienerschnitzel', 'law', 'pissd', 'whi', 'fell', 'employe', 'neglig', 'unwelcom', 'consist', 'offici', 'kabuki', 'over-pr', 'over-hip', 'under-servic', 'articl', 'spice', 'fuck', 'oyster', 'qualifi', 'otherwis', 'coconut', 'fella', 'huevo', 'ranchero', 'food/servic', 'temp', 'deep', 'oil', 'colleg', 'besid', 'costco', 'groceri', 'doughi', 'salsa', 'albondiga', 'tomato', 'meatbal', 'occas', 'bloodiest', 'anymor']\n",
            "3000\n",
            "then i n't wast enough life pour salt wound draw time took bring check\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "the case great work fine 680\n",
            "[0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Writing sentence: the case great work fine 680\n",
            "Writing sentence: then i n't wast enough life pour salt wound draw time took bring check\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cQ7T4Zwbymw",
        "colab_type": "text"
      },
      "source": [
        "# E: Post Processing\n",
        "Since the vast majority of English words will not appear in\n",
        "most of the reviews, most of the feature vector elements will be 0. This suggests that we need\n",
        "a postprocessing or normalization strategy that combats the huge variance of the elements\n",
        "in the feature vector. You may want to use one of the following strategies. Whatever choices\n",
        "you make, explain why you made the decision.\n",
        "\n",
        "• log-normalization f (x) =log (x +1).\n",
        "\n",
        "• l1 normalization. x =x / | x |\n",
        "\n",
        "• l2 normalization. x = x / ||x||\n",
        "\n",
        "• Standardize the data by subtracting the mean and dividing by the variance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViT08HCzK6VE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Log normalize the data\n",
        "# feature_matrix = np.log(np.array(feature_matrix), where=feature_matrix>0)\n",
        "\n",
        "from math import log\n",
        "\n",
        "def log_norm(feature_matrix):\n",
        "    feature_matrix_post = []\n",
        "    for idx, feature in enumerate(feature_matrix):\n",
        "        # print(feature)\n",
        "        for jdx, f in enumerate(feature):\n",
        "            l = log(1+f)\n",
        "            # print(l)\n",
        "            feature[jdx] = l\n",
        "        # feature_matrix[idx] = feature\n",
        "        feature_matrix_post.append(feature)\n",
        "    return feature_matrix_post\n",
        "\n",
        "feature_matrix_post = log_norm(feature_matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOT6mW9wE3ke",
        "colab_type": "text"
      },
      "source": [
        "# F: Sentiment Prediction\n",
        "Train a logistic regression model (you can use existing packages here)\n",
        "on the training set and test on the testing set. Report the classification accuracy and confusion matrix. Inspecting the weight vector of the logistic regression, what are the words that\n",
        "play the most important roles in deciding the sentiment of the reviews? Repeat this with a\n",
        "Naive Bayes classifier and compare performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIzycFChFCUB",
        "colab_type": "code",
        "outputId": "8b749159-c06b-4370-ce59-5597b3e73e5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "# Logistic Regression prediction\n",
        "lr_bow = LogisticRegression()\n",
        "\n",
        "# Convert to np arrays\n",
        "# print(X_train)\n",
        "X_train_bow = np.array(feature_matrix_post[:2400]) # First 2400 values of feature matrix are training data (positive and negative)\n",
        "lr_bow.fit(X_train_bow, Y_train) # Original Y_train is already set\n",
        "\n",
        "X_test_bow = np.array(feature_matrix_post[2400:])\n",
        "print(len(X_test_bow))\n",
        "\n",
        "y_pred = lr_bow.predict(X_test_bow)\n",
        "y_score = lr_bow.score(X_test_bow, Y_test)\n",
        "# print(y_pred)\n",
        "print(y_score)\n",
        "# Generate confusion matrix from logistic regression\n",
        "c_matrix = confusion_matrix(Y_test, y_pred)\n",
        "print(c_matrix)\n",
        "\n",
        "# Gaussian Naive Bayes \n",
        "gnb = GaussianNB()\n",
        "# y_pred = gnb.fit(X_train_phrases, Y_train).predict(X_train_phrases)\n",
        "gnb.fit(X_train_bow, Y_train)\n",
        "gnb_score = gnb.score(X_test_bow, Y_test)\n",
        "print(\"Gaussian Naive Bayes Score: {}\".format(gnb_score))\n",
        "\n",
        "# Bernoulli Naive Bayes\n",
        "bnb = BernoulliNB()\n",
        "# y_pred = bnb.fit(X_train_phrases, Y_train).predict(X_train_phrases)\n",
        "bnb.fit(X_train_bow, Y_train)\n",
        "bnb_score = bnb.score(X_test_bow, Y_test)\n",
        "bnb_pred = bnb.predict(X_test_bow)\n",
        "bnb_matrix = confusion_matrix(Y_test, bnb_pred)\n",
        "print(bnb_matrix)\n",
        "print(\"Bernoulli Naive Bayes Score: {}\".format(bnb_score))\n",
        "\n",
        "# Multinomial Naive Bayes\n",
        "mnb = MultinomialNB()\n",
        "mnb.fit(X_train_bow, Y_train)\n",
        "mnb_score = mnb.score(X_test_bow, Y_test)\n",
        "mnb_pred = mnb.predict(X_test_bow)\n",
        "print(\"Multinomial Naive Bayes Score: {}\".format(mnb_score))\n",
        "mnb_matrix = confusion_matrix(Y_test, mnb_pred)\n",
        "print(mnb_matrix)\n",
        "\n",
        "print(mnb.coef_[0])\n",
        "\n",
        "print(bnb.coef_[0])"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "600\n",
            "0.8233333333333334\n",
            "[[262  38]\n",
            " [ 68 232]]\n",
            "Gaussian Naive Bayes Score: 0.65\n",
            "[[257  43]\n",
            " [ 65 235]]\n",
            "Bernoulli Naive Bayes Score: 0.82\n",
            "Multinomial Naive Bayes Score: 0.8133333333333334\n",
            "[[249  51]\n",
            " [ 61 239]]\n",
            "[-4.61059354 -6.63127927 -5.72949555 ... -9.17943471 -9.17943471\n",
            " -9.17943471]\n",
            "[-2.19390232 -4.20137036 -3.30755248 ... -7.09174212 -7.09174212\n",
            " -7.09174212]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eq9FblVNL144",
        "colab_type": "text"
      },
      "source": [
        "# G: N-Gram Model\n",
        "N-gram model. Similar to the bag of words model, but now you build up a dictionary of ngrams, which are contiguous sequences of words. For example, “Alice fell down the rabbit\n",
        "hole” would then map to the 2-grams sequence: [\"Alice fell\", \"fell down\", \"down the\", \"the\n",
        "rabbit\", \"rabbit hole\"], and all five of those symbols would be members of the n-gram dictionary. Try n = 2, repeat (d)-(g) and report your results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "em63uaNqBPXg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "e1919d4a-b472-4b80-f145-1a4f0d204fe5"
      },
      "source": [
        "# Create N-gram model from sentence string\n",
        "\n",
        "s = \"Alice fell down the rabbit hole twice\"\n",
        "n = 2\n",
        "\n",
        "def generate_n_gram(s, n):\n",
        "    split = s.split()\n",
        "    n_gram = [None] * (len(split) - 1)\n",
        "    for i in range(len(n_gram)):\n",
        "        n_gram[i] = \" \".join(split[i:i+2])\n",
        "\n",
        "    return n_gram\n",
        "    # print(n_gram)\n",
        "\n",
        "generate_n_gram(s, n)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Alice fell',\n",
              " 'fell down',\n",
              " 'down the',\n",
              " 'the rabbit',\n",
              " 'rabbit hole',\n",
              " 'hole twice']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQGCZXRYDmnV",
        "colab_type": "text"
      },
      "source": [
        "# Bag of Phrases Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjnkFwcdM2E9",
        "colab_type": "code",
        "outputId": "c3378775-acc3-4d69-d924-a85827db9c05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Gram size of 2\n",
        "n = 2\n",
        "\n",
        "# Generate a dictionary of grams from training set\n",
        "all_grams = {}\n",
        "for r in X_train:\n",
        "    # Generate an n gram\n",
        "    phrases = generate_n_gram(r, n)\n",
        "    # print(phrases)\n",
        "    # Initialize feature vector dict with just the phrases and 0 count\n",
        "    for p in phrases:\n",
        "        if p not in all_grams:\n",
        "            all_grams[p] = 0\n",
        "\n",
        "unique_phrases_list = list(all_grams.keys())\n",
        "# print(\"Unique phrases({}): {}\".format(len(unique_phrases_list), unique_phrases_list))\n",
        "# unique_words_list = list(all_words.keys())\n",
        "# print(\"Unique words({}): {}\".format(len(unique_words_list), unique_words_list))\n",
        "\n",
        "# Generate Bag of Phrases\n",
        "test = [\"this phone is a good phone\"]\n",
        "# Create a feature matrix of all review's bag of words feature vectors\n",
        "feature_matrix_phrases = []\n",
        "all_reviews = X_train + X_test\n",
        "# all_reviews = test\n",
        "for r in all_reviews:\n",
        "    # print(\"Looking at review: {}\".format(r))\n",
        "    review_phrases = generate_n_gram(r, n)\n",
        "    feature_vector_phrases = [0] * len(unique_phrases_list)\n",
        "    for p in review_phrases:\n",
        "        if p in unique_phrases_list:\n",
        "            # print(\"Phrase '{}' found in unique list\".format(p))\n",
        "            idx = unique_phrases_list.index(p)\n",
        "            feature_vector_phrases[idx] += 1\n",
        "    # print(feature_vector_phrases)\n",
        "    feature_matrix_phrases.append(feature_vector_phrases)\n",
        "\n",
        "print(len(feature_matrix_phrases))\n",
        "print(len(feature_matrix_phrases[0]))\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3000\n",
            "12663\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_x2eKuvDwlp",
        "colab_type": "text"
      },
      "source": [
        "# Bag of Phrases Post Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fN4yfK_jDiGz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#****************************\n",
        "# TODO: Post Processing\n",
        "#****************************\n",
        "\n",
        "feature_matrix_phrases_post = log_norm(feature_matrix_phrases)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqFmeV9DDzsD",
        "colab_type": "text"
      },
      "source": [
        "# Bag of Phrases Sentiment Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOVr6Ya4Djy8",
        "colab_type": "code",
        "outputId": "47fc0612-ac45-4256-c800-8bdaa8999c69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "#***************************************\n",
        "# TODO: Sentiment Prediction for Phrases\n",
        "#***************************************\n",
        "# Logistic Regression prediction\n",
        "lr_phrases = LogisticRegression(solver=\"lbfgs\")\n",
        "\n",
        "# Convert to np arrays\n",
        "X_train_phrases = np.array(feature_matrix_phrases_post[:2400]) # First 2400 values of feature matrix are training data (positive and negative)\n",
        "lr_phrases.fit(X_train_phrases, Y_train) # Original Y_train is already set\n",
        "print(X_train_phrases.shape)\n",
        "\n",
        "X_test_phrases = np.array(feature_matrix_phrases_post[2400:])\n",
        "print(len(X_test_phrases))\n",
        "\n",
        "y_pred = lr_phrases.predict(X_test_phrases)\n",
        "y_score = lr_phrases.score(X_test_phrases, Y_test)\n",
        "# np.savetxt(\"whats_cooking_results.csv\", y_pred, delimiter(\",\"))\n",
        "\n",
        "lr_phrases_matrix = confusion_matrix(Y_test, y_pred)\n",
        "print(lr_phrases_matrix)\n",
        "print(y_score)\n",
        "\n",
        "# Bernoulli Naive Bayes\n",
        "bnb = BernoulliNB()\n",
        "# y_pred = bnb.fit(X_train_phrases, Y_train).predict(X_train_phrases)\n",
        "bnb.fit(X_train_phrases, Y_train)\n",
        "bnb_score = bnb.score(X_test_phrases, Y_test)\n",
        "bnb_pred_phrases = bnb.predict(X_test_phrases)\n",
        "bnb_phrases_matrix = confusion_matrix(Y_test, bnb_pred_phrases)\n",
        "print(\"Bernoulli Naive Bayes Score: {}\".format(bnb_score))\n",
        "print(bnb_phrases_matrix)\n",
        "# print(cross_val_score(bnb, X_train_phrases, Y_train, cv=3))\n",
        "\n",
        "# Multinomial Naive Bayes\n",
        "mnb_phrases = MultinomialNB()\n",
        "mnb_phrases.fit(X_train_phrases, Y_train)\n",
        "mnb_phrases_score = mnb_phrases.score(X_test_phrases, Y_test)\n",
        "mnb_phrases_pred = mnb_phrases.predict(X_test_phrases)\n",
        "print(\"Multinomial Naive Bayes Score: {}\".format(mnb_phrases_score))\n",
        "mnb_phrases_matrix = confusion_matrix(Y_test, mnb_phrases_pred)\n",
        "print(mnb_phrases_matrix)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2400, 12663)\n",
            "600\n",
            "[[258  42]\n",
            " [159 141]]\n",
            "0.665\n",
            "Bernoulli Naive Bayes Score: 0.6816666666666666\n",
            "[[260  40]\n",
            " [151 149]]\n",
            "Multinomial Naive Bayes Score: 0.6766666666666666\n",
            "[[253  47]\n",
            " [147 153]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nq-iIXdCGsgG",
        "colab_type": "text"
      },
      "source": [
        "# H: PCA for Bag of Words Model\n",
        "PCA for bag of words model. The features in the bag of words model have large redundancy.\n",
        "Implement PCA to reduce the dimension of features calculated in (e) to 10, 50 and 100 respectively. Using these lower-dimensional feature vectors and repeat (f ), (g). Report corresponding clustering and classification results. (Note: You should implement PCA yourself,\n",
        "but you can use numpy.svd or some other SVD package. Feel free to double-check your PCA\n",
        "implementation against an existing one)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iA8mEKDsgnLQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pca_with_svd(matrix, num_features):\n",
        "    # print(\"*******************\")\n",
        "    # print(\"*PCA With SVD IPML*\")\n",
        "    # print(\"*******************\")\n",
        "    matrix_mean = np.mean(matrix.T, axis=1)\n",
        "    # print(\"Mean: {}\".format(matrix_mean))\n",
        "    centered = matrix - matrix_mean\n",
        "    # print(\"Centered shape: {}\".format(centered.shape))\n",
        "\n",
        "    # TODO: Don't need to center data if we use SVD?\n",
        "    # U, Sigma, V_T = np.linalg.svd(matrix)\n",
        "    U, Sigma, V_T = np.linalg.svd(centered)\n",
        "\n",
        "    top_evals = Sigma[:num_features]\n",
        "    # print(\"Eigenvalues shape: {}\".format(top_evals.shape))\n",
        "    top_evectors = V_T[:num_features] # Use transpose of V_T\n",
        "    # print(\"Evectors shape: {}\".format(top_evectors.shape))\n",
        "\n",
        "    # Project onto new feature space\n",
        "    # Y = centered.T.dot(top_evectors.T)\n",
        "    # result = centered.dot(top_evectors.T)\n",
        "    result = np.matmul(centered, top_evectors.T)\n",
        "    # print(\"result Shape: {}\".format(result.shape))\n",
        "    # return Y.T\n",
        "    return result\n",
        "\n",
        "# result_train = pca_with_svd(X_train_bow, 10)\n",
        "# result_test = pca_with_svd(X_test_bow, 10)\n",
        "\n",
        "# Perform logistic regression on BoW model after PCA on train and test data\n",
        "# lr = LogisticRegression(solver=\"lbfgs\")\n",
        "# lr.fit(result_train, Y_train)\n",
        "# y_pred = lr.predict(result_test)\n",
        "# y_score = lr.score(result_test, Y_test)\n",
        "# print(\"BOW Logistic Regression Score: {}\".format(y_score))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80UMd0cDKJnT",
        "colab_type": "code",
        "outputId": "20131150-6e95-4d92-83ee-59bc00d940a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Fit PCA to features calculated after post processing step\n",
        "# X_train_bow; X_test_bow; Y_train; Y_test\n",
        "\n",
        "# *********************************************\n",
        "# *** TESTING PCA IMPLEMENTATION VS SKLEARN ***\n",
        "# *********************************************\n",
        "# Perform PCA on 10, 50, and 100 features\n",
        "num_features = [10, 50, 100]\n",
        "# num_features = [10]\n",
        "use_my_pca = True\n",
        "for f in num_features:\n",
        "    print(\"####################################\")\n",
        "    print(\"Number of PCA Features: {}\".format(f))\n",
        "    print(\"####################################\")\n",
        "    print(\"Using My PCA? {}\".format(use_my_pca))\n",
        "    if use_my_pca:\n",
        "        X_train_bow_pca = pca_with_svd(X_train_bow, f)\n",
        "        X_test_bow_pca = pca_with_svd(X_test_bow, f)\n",
        "        X_train_phrases_pca = pca_with_svd(X_train_phrases, f)\n",
        "        X_test_phrases_pca = pca_with_svd(X_test_phrases, f)\n",
        "    else:\n",
        "        pca = PCA(n_components=f)\n",
        "        pca.fit(X_train_bow)\n",
        "        print(\"Number of PCA components: {}\".format(pca.n_components_))\n",
        "        X_train_bow_pca = pca.transform(X_train_bow)\n",
        "        X_test_bow_pca = pca.transform(X_test_bow)\n",
        "\n",
        "    # Perform logistic regression on BoW model after PCA on train and test data\n",
        "    lr = LogisticRegression(solver=\"lbfgs\")\n",
        "    lr.fit(X_train_bow_pca, Y_train)\n",
        "    y_pred = lr.predict(X_test_bow_pca)\n",
        "    y_score = lr.score(X_test_bow_pca, Y_test)\n",
        "    lr_bow_matrix = confusion_matrix(Y_test, y_pred)\n",
        "    print(\"BOW Logistic Regression Score: {}\".format(y_score))\n",
        "    print(\"LR Confusion BOW: {}\".format(lr_bow_matrix))\n",
        "\n",
        "    # Bernoulli Naive Bayes\n",
        "    bnb_bow = BernoulliNB()\n",
        "    bnb_bow.fit(X_train_bow_pca, Y_train)\n",
        "    bnb_bow_score = bnb_bow.score(X_test_bow_pca, Y_test)\n",
        "    bnb_bow_pred = bnb_bow.predict(X_test_bow_pca)\n",
        "    print(\"BOW Bernoulli Naive Bayes Score: {}\".format(bnb_bow_score))\n",
        "    bnb_bow_matrix = confusion_matrix(Y_test, bnb_bow_pred)\n",
        "    print(\"BNB BoW Matrix: {}\".format(bnb_bow_matrix))\n",
        "\n",
        "    # PCA For Bag of Phrases\n",
        "    # pca = PCA(n_components=f)\n",
        "    # pca.fit(X_train_phrases)\n",
        "    # X_train_phrases_pca = pca.transform(X_train_phrases)\n",
        "    # X_test_phrases_pca = pca.transform(X_test_phrases)\n",
        "\n",
        "    # Phrases Logistic Regression \n",
        "    lr = LogisticRegression(solver=\"lbfgs\")\n",
        "    lr.fit(X_train_phrases_pca, Y_train) # Original Y_train is already set\n",
        "    y_pred = lr.predict(X_test_phrases_pca)\n",
        "    y_score = lr.score(X_test_phrases_pca, Y_test)\n",
        "    print(\"Phrases Logistic Regression Score: {}\".format(y_score))\n",
        "    lr_phrases_matrix = confusion_matrix(Y_test, y_pred)\n",
        "    print(\"Phrases LR Phrases Matrix: {}\".format(lr_phrases_matrix))\n",
        "\n",
        "    # Bernoulli Naive Bayes\n",
        "    bnb_phrases = BernoulliNB()\n",
        "    bnb_phrases.fit(X_train_phrases_pca, Y_train)\n",
        "    bnb_phrases_score = bnb_phrases.score(X_test_phrases_pca, Y_test)\n",
        "    bnb_phrases_pred = bnb_phrases.predict(X_test_phrases_pca)\n",
        "    print(\"Phrases Bernoulli Naive Bayes Score: {}\".format(bnb_phrases_score))\n",
        "    bnb_phrases_matrix = confusion_matrix(Y_test, bnb_phrases_pred)\n",
        "    print(\"Phrases BNB Phrases Matrix: {}\".format(bnb_phrases_matrix))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "####################################\n",
            "Number of PCA Features: 10\n",
            "####################################\n",
            "Using My PCA? True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUbU3eGdfnmb",
        "colab_type": "text"
      },
      "source": [
        "# I: Algorithms Comparison and Analysis\n",
        "Algorithms comparison and analysis. According to the above results, compare the performances of bag of words, 2-gram and PCA for bag of words. Which method performs best in\n",
        "the prediction task and why? What do you learn about the language that people use in online reviews (e.g., expressions that will make the posts positive/negative)? Hint: Inspect the\n",
        "clustering results and the weights learned from logistic regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqw3VJy7fs4y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "f815d126-c4f1-4dd1-db6c-b28dc18898d7"
      },
      "source": [
        "# Look at Logistic Regression Weights\n",
        "# Map the LR feature weights to the feature matrix indices, which will tell us the\n",
        "# most discriminative words\n",
        "\n",
        "# print(lr_bow.coef_.shape)\n",
        "# print(lr_bow.coef_[0])\n",
        "\n",
        "NUMBER_OF_FEATURES = 10\n",
        "print(\"Looking at {} top features from Logistic Regression Weights\".format(NUMBER_OF_FEATURES))\n",
        "\n",
        "# Get max coefficients\n",
        "lr_bow_sorted = sorted(lr_bow.coef_[0], key=abs, reverse=True)\n",
        "# print(lr_bow_sorted)\n",
        "# Top N values\n",
        "top_features = lr_bow_sorted[:NUMBER_OF_FEATURES]\n",
        "# print(top_features)\n",
        "\n",
        "indicies = []\n",
        "# Find indicies of top 10 values\n",
        "for v in top_features:\n",
        "    idx = np.where(lr_bow.coef_[0] == v)\n",
        "    # print(\"Index of {} is {}\".format(v, idx[0][0]))\n",
        "    indicies.append(idx[0][0])\n",
        "\n",
        "# print(lr_bow.coef_[0][4])\n",
        "\n",
        "# Map these indicies into our original feature vector for getting the discriminative words\n",
        "for i in indicies:\n",
        "    word = unique_words_list[i]\n",
        "    print(\"Disciminative word at {} is '{}'\".format(i, word))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking at 10 top features from Logistic Regression Weights\n",
            "Disciminative word at 4 is 'great'\n",
            "Disciminative word at 858 is 'bad'\n",
            "Disciminative word at 38 is 'love'\n",
            "Disciminative word at 849 is 'poor'\n",
            "Disciminative word at 2 is 'excel'\n",
            "Disciminative word at 2999 is 'delici'\n",
            "Disciminative word at 86 is 'nice'\n",
            "Disciminative word at 978 is 'worst'\n",
            "Disciminative word at 554 is 'amaz'\n",
            "Disciminative word at 288 is 'fantast'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3O9sjKFgkrl0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "f6a152c5-7840-4da7-da2d-8ad86d528953"
      },
      "source": [
        "# Look at weights and words for Multinomial Naive Bayes\n",
        "\n",
        "NUMBER_OF_FEATURES = 10\n",
        "print(\"Looking at {} top features from Multinomial Naive Bayes Weights\".format(NUMBER_OF_FEATURES))\n",
        "\n",
        "# Get max coefficients\n",
        "nb_bow_sorted = sorted(mnb.coef_[0], reverse=True)\n",
        "# print(lr_bow_sorted)\n",
        "# Top N values\n",
        "top_features = nb_bow_sorted[:NUMBER_OF_FEATURES]\n",
        "print(top_features)\n",
        "\n",
        "indicies = []\n",
        "# Find indicies of top 10 values\n",
        "for v in top_features:\n",
        "    idx = np.where(mnb.coef_[0] == v)\n",
        "    # print(\"Index of {} is {}\".format(v, idx[0][0]))\n",
        "    indicies.append(idx[0][0])\n",
        "\n",
        "# print(lr_bow.coef_[0][4])\n",
        "\n",
        "# Map these indicies into our original feature vector for getting the discriminative words\n",
        "for i in indicies:\n",
        "    word = unique_words_list[i]\n",
        "    print(\"Disciminative word at {} is '{}'\".format(i, word))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking at 10 top features from Multinomial Naive Bayes Weights\n",
            "[-3.7189655614746924, -4.3450077554921505, -4.436750281773733, -4.610593543954772, -4.904374566510776, -5.045830700402312, -5.085370563015636, -5.121787520222974, -5.177101603132868, -5.198353031431271]\n",
            "Disciminative word at 32 is 'i'\n",
            "Disciminative word at 6 is 'the'\n",
            "Disciminative word at 4 is 'great'\n",
            "Disciminative word at 0 is 'good'\n",
            "Disciminative word at 44 is ''s'\n",
            "Disciminative word at 60 is 'this'\n",
            "Disciminative word at 1369 is 'film'\n",
            "Disciminative word at 78 is 'it'\n",
            "Disciminative word at 28 is 'phone'\n",
            "Disciminative word at 1346 is 'movi'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJN28MiJCgd_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "f3d6b011-d626-4d44-9688-4c9369e56a96"
      },
      "source": [
        "# Look at Logistic Regression weights for Bag of Phrases Model\n",
        "\n",
        "# print(lr_phrases.coef_.shape)\n",
        "# print(lr_phrases.coef_[0])\n",
        "\n",
        "NUMBER_OF_FEATURES = 10\n",
        "print(\"Looking at {} top features from Logistic Regression Weights\".format(NUMBER_OF_FEATURES))\n",
        "# Get max coefficients\n",
        "lr_phrases_sorted = sorted(lr_phrases.coef_[0], key=abs, reverse=True)\n",
        "# print(lr_phrases_sorted)\n",
        "# Top N values\n",
        "top_features = lr_phrases_sorted[:NUMBER_OF_FEATURES]\n",
        "# print(top_features)\n",
        "\n",
        "indicies = []\n",
        "# Find indicies of top 10 values\n",
        "for v in top_features:\n",
        "    idx = np.where(lr_phrases.coef_[0] == v)\n",
        "    # print(\"Index of {} is {}\".format(v, idx[0][0]))\n",
        "    indicies.append(idx[0][0])\n",
        "\n",
        "# print(lr_bow.coef_[0][4])\n",
        "\n",
        "# Map these indicies into our original feature vector for getting the discriminative words\n",
        "for i in indicies:\n",
        "    word = unique_phrases_list[i]\n",
        "    print(\"Disciminative phrase at {} is '{}'\".format(i, word))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking at 10 top features from Logistic Regression Weights\n",
            "Disciminative phrase at 133 is 'i love'\n",
            "Disciminative phrase at 29 is 'work great'\n",
            "Disciminative phrase at 1355 is 'i like'\n",
            "Disciminative phrase at 21 is 'high recommend'\n",
            "Disciminative phrase at 245 is 'one best'\n",
            "Disciminative phrase at 2110 is 'wast time'\n",
            "Disciminative phrase at 480 is 'great phone'\n",
            "Disciminative phrase at 9946 is 'the servic'\n",
            "Disciminative phrase at 317 is 'great product'\n",
            "Disciminative phrase at 156 is 'i realli'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGEnK2syoj4j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "791d0e9f-05f3-467d-c6a0-5d27647c5f50"
      },
      "source": [
        "# Look at Naive Bayes  weights for Bag of Phrases Model\n",
        "\n",
        "# print(lr_phrases.coef_.shape)\n",
        "# print(lr_phrases.coef_[0])\n",
        "\n",
        "NUMBER_OF_FEATURES = 60\n",
        "print(\"Looking at {} top features from Naive Bayes Phrases Weights\".format(NUMBER_OF_FEATURES))\n",
        "# Get max coefficients\n",
        "nb_phrases_sorted = sorted(mnb_phrases.coef_[0], reverse=True)\n",
        "# print(lr_phrases_sorted)\n",
        "# Top N values\n",
        "top_features = nb_phrases_sorted[:NUMBER_OF_FEATURES]\n",
        "# print(top_features)\n",
        "\n",
        "indicies = []\n",
        "# Find indicies of top 10 values\n",
        "for v in top_features:\n",
        "    idx = np.where(mnb_phrases.coef_[0] == v)\n",
        "    # print(\"Index of {} is {}\".format(v, idx[0][0]))\n",
        "    if idx[0][0] not in indicies:\n",
        "        indicies.append(idx[0][0])\n",
        "\n",
        "# print(lr_bow.coef_[0][4])\n",
        "\n",
        "# Map these indicies into our original feature vector for getting the discriminative words\n",
        "for i in indicies:\n",
        "    word = unique_phrases_list[i]\n",
        "    print(\"Disciminative phrase at {} is '{}'\".format(i, word))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking at 60 top features from Naive Bayes Phrases Weights\n",
            "Disciminative phrase at 133 is 'i love'\n",
            "Disciminative phrase at 52 is 'i ve'\n",
            "Disciminative phrase at 29 is 'work great'\n",
            "Disciminative phrase at 612 is 'it 's'\n",
            "Disciminative phrase at 1355 is 'i like'\n",
            "Disciminative phrase at 169 is 'i think'\n",
            "Disciminative phrase at 21 is 'high recommend'\n",
            "Disciminative phrase at 388 is 'i 'm'\n",
            "Disciminative phrase at 245 is 'one best'\n",
            "Disciminative phrase at 156 is 'i realli'\n",
            "Disciminative phrase at 10 is 'sound qualiti'\n",
            "Disciminative phrase at 1090 is 'i n't'\n",
            "Disciminative phrase at 798 is 'time i'\n",
            "Disciminative phrase at 317 is 'great product'\n",
            "Disciminative phrase at 18 is 'veri good'\n",
            "Disciminative phrase at 19 is 'good qualiti'\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}